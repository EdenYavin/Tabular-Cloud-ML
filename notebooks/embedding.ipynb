{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-07T05:58:19.937805Z",
     "start_time": "2025-05-07T05:58:13.191644Z"
    }
   },
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T05:58:19.951136Z",
     "start_time": "2025-05-07T05:58:19.947128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess(X: pd.DataFrame, cloud_dataset=False):\n",
    "    \"\"\"\n",
    "    The function will preprocess the data:\n",
    "    1. Categorical features will be label encoded (Boy->1, Girl ->2)\n",
    "    2. Numerical features will be scaled if the data is intended to be used for baseline. For cloud data set, no scaling will be preformed.\n",
    "\n",
    "    Return pd.Dataframe\n",
    "    \"\"\"\n",
    "    # Identify categorical and numeric columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numeric_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "    # Initialize lists to store processed columns\n",
    "    processed_columns = []\n",
    "\n",
    "    # If there are categorical columns, apply one-hot encoding\n",
    "    if categorical_cols:\n",
    "        print(\"\\nEncoding categorical columns...\")\n",
    "        X_categorical = pd.get_dummies(X[categorical_cols], drop_first=True)\n",
    "        # label_encoder = LabelEncoder()\n",
    "        # X_categorical = pd.DataFrame()\n",
    "        # for col in categorical_cols:\n",
    "        #     # X_categorical[col] = label_encoder.fit_transform(X[col])\n",
    "        processed_columns.append(X_categorical)\n",
    "\n",
    "    # Apply standard scaling to the numeric columns\n",
    "    if numeric_cols:\n",
    "        print(\"\\nScaling numerical columns...\")\n",
    "        scaler = MinMaxScaler()\n",
    "        # X_numeric = X[numeric_cols]\n",
    "        # if cloud_dataset:\n",
    "        X_numeric = pd.DataFrame(scaler.fit_transform(X[numeric_cols]), columns=numeric_cols, index=X.index)\n",
    "        # else:\n",
    "        #     X_numeric = pd.DataFrame(scaler.fit_transform(X[numeric_cols]), columns=numeric_cols, index=X.index)\n",
    "\n",
    "        processed_columns.append(X_numeric)\n",
    "\n",
    "    # Combine the processed columns\n",
    "    if processed_columns:\n",
    "        X_processed = pd.concat(processed_columns, axis=1)\n",
    "    else:\n",
    "        X_processed = X.copy()  # If there are no categorical or numeric columns, keep the original dataframe\n",
    "\n",
    "\n",
    "    return X_processed"
   ],
   "id": "c2ea84c2040e76ac",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:01:31.007350Z",
     "start_time": "2025-05-07T06:01:31.002096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RawDataset:\n",
    "    name: str\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.X, self.y = None, None\n",
    "        self.sample_split = 1\n",
    "        self.baseline_model = \"neural_network\"\n",
    "        self.name = None\n",
    "        self.metadata = {}\n",
    "\n",
    "    def get_n_classes(self):\n",
    "        return len(np.unique(self.y))\n",
    "\n",
    "    def get_number_of_features(self):\n",
    "        return self.X.shape[1]\n",
    "\n",
    "    def get_dataset(self):\n",
    "        return self.X, self.y\n",
    "\n",
    "\n",
    "    def _get_model(self, X_train, y_train):\n",
    "        inputs = Input(shape=(X_train.shape[1],))  # Dynamic input shape\n",
    "\n",
    "        # Define the hidden layers\n",
    "        x = BatchNormalization()(inputs)\n",
    "        x = Dense(units=128, activation='leaky_relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        # x = Dense(units=64, activation='leaky_relu')(x)\n",
    "        # x = Dropout(0.3)(x)\n",
    "\n",
    "        # Define the output layer\n",
    "        outputs = Dense(units=len(np.unique(y_train)), activation='softmax')(x)\n",
    "\n",
    "        # Create the model\n",
    "        clf = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # Compile the model with F1 Score\n",
    "        clf.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      )\n",
    "\n",
    "        return clf"
   ],
   "id": "327c159ff28f0f7c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:01:31.935781Z",
     "start_time": "2025-05-07T06:01:31.783254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pmlb import fetch_data\n",
    "\n",
    "class PMLBDataset(RawDataset):\n",
    "\n",
    "    def __init__(self,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        dataset_name = \"ring\"\n",
    "\n",
    "        dataset = fetch_data(dataset_name)\n",
    "        self.X, self.y = self._preprocess(dataset)\n",
    "        self.cloud_models = kwargs.get(\"cloud_models\")\n",
    "        self.name = dataset_name\n",
    "        self.metadata[\"labels\"] = [0, 1]\n",
    "        self.metadata['targe_column'] = \"target\"\n",
    "\n",
    "    def _preprocess(self, dataset: pd.DataFrame):\n",
    "        dataset = dataset.dropna().sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        X,y = dataset.drop(columns=[\"target\"]), dataset[\"target\"]\n",
    "        return preprocess(X, cloud_dataset=True), y"
   ],
   "id": "5a8a033cd2a68b7d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:01:32.893234Z",
     "start_time": "2025-05-07T06:01:32.890094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HelocDataset(RawDataset):\n",
    "    name = 'heloc'\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        DATASET_PATH = Path(\"..\") / \"data\" / \"heloc\" / f\"dataset.csv\"\n",
    "        dataset = pd.read_csv(DATASET_PATH)\n",
    "        if \"Unnamed: 0\" in dataset.columns:\n",
    "            dataset.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "        self.X, self.y = self._preprocess(dataset)\n",
    "        self.cloud_models = kwargs.get(\"cloud_models\")\n",
    "        self.name = HelocDataset.name\n",
    "        self.metadata[\"labels\"] = [\"Bad\", \"Good\"]\n",
    "        self.metadata['targe_column'] = \"RiskPerformance\"\n",
    "        self.metadata['description'] = \"The HELOC dataset from FICO. Each entry in the dataset is a line of credit, typically offered by a bank as a percentage of home equity (the difference between the current market value of a home and its purchase price). The task is a binary classification task.\"\n",
    "\n",
    "    def _preprocess(self, dataset):\n",
    "        X, y = dataset.iloc[:, 1:], dataset.iloc[:, 0]\n",
    "\n",
    "        # Remove the bug in the dataset where the entire row has -9 values\n",
    "        mask = ~(X == -9).all(axis=1)\n",
    "        X = X[mask]\n",
    "        y = y[mask]\n",
    "\n",
    "        y = y.replace({\"Bad\": 0, \"Good\": 1}).astype(int)\n",
    "        return preprocess(X, cloud_dataset=True), y"
   ],
   "id": "6b3bceb95c0150f6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:01:37.038618Z",
     "start_time": "2025-05-07T06:01:34.126746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dataset = HelocDataset()\n",
    "dataset = PMLBDataset()"
   ],
   "id": "55e8f58b4a9fef6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaling numerical columns...\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:01:37.111583Z",
     "start_time": "2025-05-07T06:01:37.106860Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.X.shape",
   "id": "765a6790bf481350",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7400, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:01:57.811447Z",
     "start_time": "2025-05-07T06:01:37.946320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from keras.src.callbacks import EarlyStopping\n",
    "from keras.src.layers import Dropout, Dense, BatchNormalization, Input, Flatten\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "\n",
    "class SparseAE(nn.Module):\n",
    "    name: str = \"sparse_ae\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SparseAE, self).__init__()\n",
    "        X = kwargs.get(\"X\")\n",
    "        self.model = self._get_trained_model(X.astype(float))\n",
    "        self.output_shape = (1, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if type(x) is pd.DataFrame:\n",
    "            x = x.to_numpy()\n",
    "\n",
    "        embedding = self.model(x)\n",
    "        return embedding\n",
    "\n",
    "    def _get_trained_model(self, X:np.ndarray | pd.DataFrame):\n",
    "\n",
    "        def sparse_loss(y_true, y_pred):\n",
    "            sparsity_level = 0.05\n",
    "            lambda_sparse = 0.1\n",
    "            mse_loss = tf.reduce_mean(keras.losses.MeanSquaredError()(y_true, y_pred))\n",
    "            hidden_layer_output = encoder(y_true)\n",
    "            mean_activation = tf.reduce_mean(hidden_layer_output, axis=0)\n",
    "\n",
    "            kl_divergence = tf.reduce_sum(sparsity_level * tf.math.log(sparsity_level / (mean_activation + 1e-10)) +\n",
    "                                          (1 - sparsity_level) * tf.math.log(\n",
    "                (1 - sparsity_level) / (1 - mean_activation + 1e-10)))\n",
    "\n",
    "            return mse_loss + lambda_sparse * kl_divergence\n",
    "\n",
    "        input_dim = X.shape[1]\n",
    "        encoding_dim = max(int(input_dim * 1.5), 64)\n",
    "        inputs = Input(shape=(input_dim,))\n",
    "        encoded = Dense(encoding_dim, activation=\"relu\")(inputs)\n",
    "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "        autoencoder = keras.Model(inputs, decoded)\n",
    "        encoder = keras.Model(inputs, encoded)\n",
    "        early_stop = EarlyStopping(patience=2, monitor=\"loss\")\n",
    "\n",
    "        autoencoder.compile(optimizer='adam', loss=sparse_loss)\n",
    "        autoencoder.fit(X, X, epochs=50, batch_size=256, shuffle=True, callbacks=[early_stop])\n",
    "\n",
    "        return encoder\n",
    "\n",
    "model = SparseAE(X=dataset.X)"
   ],
   "id": "9d3858b2e47bb1b6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eden.yavin/Projects/Tabular-Cloud-ML/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 09:01:56.596521: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-05-07 09:01:56.596586: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 36.00 GB\n",
      "2025-05-07 09:01:56.596593: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 13.50 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746597716.596948 1401415 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1746597716.596999 1401415 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-05-07 09:01:56.935947: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - loss: nan\n",
      "Epoch 2/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: nan\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:01:58.340068Z",
     "start_time": "2025-05-07T06:01:57.814419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class RawDataExperimentDatabase:\n",
    "    \"\"\"\n",
    "    This will enable consistent experiments - Each split of dataset will be save, i.e. the indexes will be saved.\n",
    "    Thus changes to the data will not result in changes to the split\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: RawDataset):\n",
    "        self.dataset = dataset\n",
    "        db_path = os.path.join(\"..\",\"data\", dataset.name)\n",
    "        os.makedirs(db_path, exist_ok=True)\n",
    "        self.db_path = os.path.join(db_path, f\"{dataset.name}_dataset.json\")\n",
    "        self.key = str(1)\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(f\"Path exists: {self.db_path}\")\n",
    "            self.db = json.load(open(self.db_path, \"r\"))\n",
    "            self.empty = False if self.key in self.db else True\n",
    "        else:\n",
    "            self.db = {1: {}}\n",
    "            self.empty = True\n",
    "\n",
    "\n",
    "    def _save(self):\n",
    "        with open(self.db_path, \"w\") as f:\n",
    "            json.dump(self.db, f)\n",
    "\n",
    "    def get_split(self):\n",
    "\n",
    "        X, y = self.dataset.get_dataset()\n",
    "\n",
    "        if self.empty:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y,\n",
    "                                                                random_state=42)\n",
    "\n",
    "\n",
    "            X_sample, y_sample = X_train, y_train\n",
    "\n",
    "\n",
    "            new_data = {}\n",
    "            new_data[\"train_index\"] = X_train.index.tolist()\n",
    "            new_data[\"imm_train_index\"] = X_sample.index.tolist()\n",
    "            new_data[\"test_index\"] = X_test.index.tolist()\n",
    "\n",
    "            self.db[self.key] = new_data\n",
    "\n",
    "            print(f\"#### CREATED NEW INDEX FOR 1 - INDEX SIZE {len(X_sample)}\")\n",
    "\n",
    "            self._save()\n",
    "\n",
    "        else:\n",
    "            indexes = self.db[self.key]\n",
    "            # Get the existing indices and create new dataframes\n",
    "            X_train = pd.DataFrame(X.loc[indexes[\"train_index\"]])\n",
    "            y_train = pd.Series(y.loc[indexes[\"train_index\"]])\n",
    "            X_sample = pd.DataFrame(X.loc[indexes[\"imm_train_index\"]])\n",
    "            y_sample = pd.Series(y.loc[indexes[\"imm_train_index\"]])\n",
    "            X_test = pd.DataFrame(X.loc[indexes[\"test_index\"]])\n",
    "            y_test = pd.Series(y.loc[indexes[\"test_index\"]])\n",
    "            print(f\"LOADED INDEX 1 - INDEX SIZE {len(X_sample)}\")\n",
    "\n",
    "        return X_train.values, X_test.values, X_sample.values, y_train.values, y_test.values, y_sample.values\n"
   ],
   "id": "bd5604df16cc38ce",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:01:58.362881Z",
     "start_time": "2025-05-07T06:01:58.348147Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, X_test, X_sample, y_train, y_test, y_sample = RawDataExperimentDatabase(dataset).get_split()",
   "id": "ab355788b6c8dc4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path exists: ../data/ring/ring_dataset.json\n",
      "LOADED INDEX 1 - INDEX SIZE 6660\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:01:58.461638Z",
     "start_time": "2025-05-07T06:01:58.432167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_emb = model.forward(X_sample)\n",
    "X_test_emb = model.forward(X_test)"
   ],
   "id": "10cca148e080e808",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:03:21.103694Z",
     "start_time": "2025-05-07T06:03:21.094989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.src.metrics import F1Score\n",
    "from keras.src.callbacks import LearningRateScheduler\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.src.models import Model\n",
    "\n",
    "class NeuralNetworkInternalModel(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.batch_size = 64\n",
    "        self.dropout_rate = 0.3\n",
    "        self.epochs = 100\n",
    "        self.model: Model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        lr_scheduler = LearningRateScheduler(lambda epoch: 0.0001 * (0.9 ** epoch))\n",
    "        early_stopping = EarlyStopping(patience=2, monitor='loss')\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = self.model.predict(X)\n",
    "        return np.argmax(prediction, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        if len(y.shape) == 2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "\n",
    "        pred = self.predict(X)\n",
    "        return accuracy_score(y, pred), f1_score(y, pred, average='weighted')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EmbeddingBaseline(NeuralNetworkInternalModel):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.name = \"neural_network\"\n",
    "        num_classes = kwargs.get(\"num_classes\")\n",
    "        input_shape = kwargs.get(\"input_shape\")\n",
    "        self.model = self.get_model(num_classes=num_classes, input_shape=input_shape)\n",
    "\n",
    "    def get_model(self, num_classes, input_shape):\n",
    "\n",
    "        if isinstance(input_shape, int):\n",
    "            input_shape = (input_shape,)\n",
    "\n",
    "        inputs = Input(shape=input_shape)  # Dynamic input shape\n",
    "\n",
    "        # Define the hidden layers\n",
    "        x = BatchNormalization()(inputs)\n",
    "        x = Dense(units=1024, activation='leaky_relu')(x)\n",
    "        x = Dropout(self.dropout_rate)(x)\n",
    "\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(units=512, activation='leaky_relu')(x)\n",
    "        x = Dropout(self.dropout_rate)(x)\n",
    "\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(units=256, activation='leaky_relu')(x)\n",
    "        x = Dropout(self.dropout_rate)(x)\n",
    "\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(units=128, activation='leaky_relu')(x)\n",
    "        x = Dropout(self.dropout_rate)(x)\n",
    "\n",
    "        # Define the output layer\n",
    "        outputs = Dense(units=num_classes, activation='softmax')(x)\n",
    "\n",
    "        # Create the model\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # Compile the model with F1 Score\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy', F1Score()]\n",
    "                      )\n",
    "\n",
    "        return model\n",
    "\n"
   ],
   "id": "1f99c748877695ac",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:03:55.468525Z",
     "start_time": "2025-05-07T06:03:24.359614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "baseline_model = EmbeddingBaseline(num_classes=2, input_shape=X_train_emb.shape[1])\n",
    "\n",
    "# Convert integer labels to one-hot encoded labels\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=2)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "# Fit the model with one-hot encoded labels\n",
    "baseline_model.fit(\n",
    "    X_train_emb, y_train_one_hot  # Use one-hot encoded labels here\n",
    ")\n"
   ],
   "id": "6996b5b3190e6519",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 24ms/step - accuracy: 0.6896 - f1_score: 0.6771 - loss: 0.6975 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 14ms/step - accuracy: 0.9196 - f1_score: 0.9194 - loss: 0.2246 - learning_rate: 9.0000e-05\n",
      "Epoch 3/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 15ms/step - accuracy: 0.9212 - f1_score: 0.9211 - loss: 0.2025 - learning_rate: 8.1000e-05\n",
      "Epoch 4/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 14ms/step - accuracy: 0.9323 - f1_score: 0.9322 - loss: 0.1718 - learning_rate: 7.2900e-05\n",
      "Epoch 5/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 14ms/step - accuracy: 0.9355 - f1_score: 0.9354 - loss: 0.1721 - learning_rate: 6.5610e-05\n",
      "Epoch 6/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 14ms/step - accuracy: 0.9319 - f1_score: 0.9319 - loss: 0.1720 - learning_rate: 5.9049e-05\n",
      "Epoch 7/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 14ms/step - accuracy: 0.9355 - f1_score: 0.9355 - loss: 0.1658 - learning_rate: 5.3144e-05\n",
      "Epoch 8/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 14ms/step - accuracy: 0.9452 - f1_score: 0.9452 - loss: 0.1517 - learning_rate: 4.7830e-05\n",
      "Epoch 9/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 14ms/step - accuracy: 0.9402 - f1_score: 0.9400 - loss: 0.1682 - learning_rate: 4.3047e-05\n",
      "Epoch 10/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 23ms/step - accuracy: 0.9418 - f1_score: 0.9417 - loss: 0.1553 - learning_rate: 3.8742e-05\n",
      "Epoch 11/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 26ms/step - accuracy: 0.9487 - f1_score: 0.9487 - loss: 0.1395 - learning_rate: 3.4868e-05\n",
      "Epoch 12/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 37ms/step - accuracy: 0.9450 - f1_score: 0.9450 - loss: 0.1525 - learning_rate: 3.1381e-05\n",
      "Epoch 13/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 53ms/step - accuracy: 0.9483 - f1_score: 0.9483 - loss: 0.1479 - learning_rate: 2.8243e-05\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:03:57.884038Z",
     "start_time": "2025-05-07T06:03:55.547950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_model.evaluate(\n",
    "    X_test_emb, y_test,\n",
    ")"
   ],
   "id": "34cd56488b415d98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m24/24\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9662162162162162, 0.9661785866128181)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fd6b37045564161d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
