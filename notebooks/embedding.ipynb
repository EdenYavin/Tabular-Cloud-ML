{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:43:39.484322Z",
     "start_time": "2025-04-30T08:43:35.807461Z"
    }
   },
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:43:39.499309Z",
     "start_time": "2025-04-30T08:43:39.494919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess(X: pd.DataFrame, cloud_dataset=False):\n",
    "    \"\"\"\n",
    "    The function will preprocess the data:\n",
    "    1. Categorical features will be label encoded (Boy->1, Girl ->2)\n",
    "    2. Numerical features will be scaled if the data is intended to be used for baseline. For cloud data set, no scaling will be preformed.\n",
    "\n",
    "    Return pd.Dataframe\n",
    "    \"\"\"\n",
    "    # Identify categorical and numeric columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numeric_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "    # Initialize lists to store processed columns\n",
    "    processed_columns = []\n",
    "\n",
    "    # If there are categorical columns, apply one-hot encoding\n",
    "    if categorical_cols:\n",
    "        print(\"\\nEncoding categorical columns...\")\n",
    "        X_categorical = pd.get_dummies(X[categorical_cols], drop_first=True)\n",
    "        # label_encoder = LabelEncoder()\n",
    "        # X_categorical = pd.DataFrame()\n",
    "        # for col in categorical_cols:\n",
    "        #     # X_categorical[col] = label_encoder.fit_transform(X[col])\n",
    "        processed_columns.append(X_categorical)\n",
    "\n",
    "    # Apply standard scaling to the numeric columns\n",
    "    if numeric_cols:\n",
    "        print(\"\\nScaling numerical columns...\")\n",
    "        scaler = MinMaxScaler()\n",
    "        # X_numeric = X[numeric_cols]\n",
    "        # if cloud_dataset:\n",
    "        X_numeric = pd.DataFrame(scaler.fit_transform(X[numeric_cols]), columns=numeric_cols, index=X.index)\n",
    "        # else:\n",
    "        #     X_numeric = pd.DataFrame(scaler.fit_transform(X[numeric_cols]), columns=numeric_cols, index=X.index)\n",
    "\n",
    "        processed_columns.append(X_numeric)\n",
    "\n",
    "    # Combine the processed columns\n",
    "    if processed_columns:\n",
    "        X_processed = pd.concat(processed_columns, axis=1)\n",
    "    else:\n",
    "        X_processed = X.copy()  # If there are no categorical or numeric columns, keep the original dataframe\n",
    "\n",
    "\n",
    "    return X_processed"
   ],
   "id": "c2ea84c2040e76ac",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:43:39.660691Z",
     "start_time": "2025-04-30T08:43:39.657204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RawDataset:\n",
    "    name: str\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.X, self.y = None, None\n",
    "        self.sample_split = 1\n",
    "        self.baseline_model = \"neural_network\"\n",
    "        self.name = None\n",
    "        self.metadata = {}\n",
    "\n",
    "    def get_n_classes(self):\n",
    "        return len(np.unique(self.y))\n",
    "\n",
    "    def get_number_of_features(self):\n",
    "        return self.X.shape[1]\n",
    "\n",
    "    def get_dataset(self):\n",
    "        return self.X, self.y\n",
    "\n",
    "\n",
    "    def _get_model(self, X_train, y_train):\n",
    "        inputs = Input(shape=(X_train.shape[1],))  # Dynamic input shape\n",
    "\n",
    "        # Define the hidden layers\n",
    "        x = BatchNormalization()(inputs)\n",
    "        x = Dense(units=128, activation='leaky_relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "\n",
    "        # x = Dense(units=64, activation='leaky_relu')(x)\n",
    "        # x = Dropout(0.3)(x)\n",
    "\n",
    "        # Define the output layer\n",
    "        outputs = Dense(units=len(np.unique(y_train)), activation='softmax')(x)\n",
    "\n",
    "        # Create the model\n",
    "        clf = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # Compile the model with F1 Score\n",
    "        clf.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      )\n",
    "\n",
    "        return clf"
   ],
   "id": "327c159ff28f0f7c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:44:05.830717Z",
     "start_time": "2025-04-30T08:44:05.710674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pmlb import fetch_data\n",
    "\n",
    "class PMLBDataset(RawDataset):\n",
    "\n",
    "    def __init__(self,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        dataset_name = \"ring\"\n",
    "\n",
    "        dataset = fetch_data(dataset_name)\n",
    "        self.X, self.y = self._preprocess(dataset)\n",
    "        self.cloud_models = kwargs.get(\"cloud_models\")\n",
    "        self.name = dataset_name\n",
    "        self.metadata[\"labels\"] = [0, 1]\n",
    "        self.metadata['targe_column'] = \"target\"\n",
    "\n",
    "    def _preprocess(self, dataset: pd.DataFrame):\n",
    "        dataset = dataset.dropna().sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        X,y = dataset.drop(columns=[\"target\"]), dataset[\"target\"]\n",
    "        return preprocess(X, cloud_dataset=True), y"
   ],
   "id": "5a8a033cd2a68b7d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:44:08.484560Z",
     "start_time": "2025-04-30T08:44:08.480104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HelocDataset(RawDataset):\n",
    "    name = 'heloc'\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        DATASET_PATH = Path(\"..\") / \"data\" / \"heloc\" / f\"dataset.csv\"\n",
    "        dataset = pd.read_csv(DATASET_PATH)\n",
    "        if \"Unnamed: 0\" in dataset.columns:\n",
    "            dataset.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "        self.X, self.y = self._preprocess(dataset)\n",
    "        self.cloud_models = kwargs.get(\"cloud_models\")\n",
    "        self.name = HelocDataset.name\n",
    "        self.metadata[\"labels\"] = [\"Bad\", \"Good\"]\n",
    "        self.metadata['targe_column'] = \"RiskPerformance\"\n",
    "        self.metadata['description'] = \"The HELOC dataset from FICO. Each entry in the dataset is a line of credit, typically offered by a bank as a percentage of home equity (the difference between the current market value of a home and its purchase price). The task is a binary classification task.\"\n",
    "\n",
    "    def _preprocess(self, dataset):\n",
    "        X, y = dataset.iloc[:, 1:], dataset.iloc[:, 0]\n",
    "\n",
    "        # Remove the bug in the dataset where the entire row has -9 values\n",
    "        mask = ~(X == -9).all(axis=1)\n",
    "        X = X[mask]\n",
    "        y = y[mask]\n",
    "\n",
    "        y = y.replace({\"Bad\": 0, \"Good\": 1}).astype(int)\n",
    "        return preprocess(X, cloud_dataset=True), y"
   ],
   "id": "6b3bceb95c0150f6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T09:06:45.569944Z",
     "start_time": "2025-04-30T09:06:44.782930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dataset = HelocDataset()\n",
    "dataset = PMLBDataset()"
   ],
   "id": "55e8f58b4a9fef6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaling numerical columns...\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T09:06:52.856563Z",
     "start_time": "2025-04-30T09:06:52.853533Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.X.shape",
   "id": "765a6790bf481350",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7400, 20)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:55:48.988761Z",
     "start_time": "2025-04-30T08:55:40.486122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from keras.src.callbacks import EarlyStopping\n",
    "from keras.src.layers import Dropout, Dense, BatchNormalization, Input, Flatten\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "\n",
    "class SparseAE(nn.Module):\n",
    "    name: str = \"sparse_ae\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SparseAE, self).__init__()\n",
    "        X = kwargs.get(\"X\")\n",
    "        self.model = self._get_trained_model(X.astype(float))\n",
    "        self.output_shape = (1, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if type(x) is pd.DataFrame:\n",
    "            x = x.to_numpy()\n",
    "\n",
    "        embedding = self.model(x)\n",
    "        return embedding\n",
    "\n",
    "    def _get_trained_model(self, X:np.ndarray | pd.DataFrame):\n",
    "\n",
    "        def sparse_loss(y_true, y_pred):\n",
    "            sparsity_level = 0.05\n",
    "            lambda_sparse = 0.1\n",
    "            mse_loss = tf.reduce_mean(keras.losses.MeanSquaredError()(y_true, y_pred))\n",
    "            hidden_layer_output = encoder(y_true)\n",
    "            mean_activation = tf.reduce_mean(hidden_layer_output, axis=0)\n",
    "\n",
    "            kl_divergence = tf.reduce_sum(sparsity_level * tf.math.log(sparsity_level / (mean_activation + 1e-10)) +\n",
    "                                          (1 - sparsity_level) * tf.math.log(\n",
    "                (1 - sparsity_level) / (1 - mean_activation + 1e-10)))\n",
    "\n",
    "            return mse_loss + lambda_sparse * kl_divergence\n",
    "\n",
    "        input_dim = X.shape[1]\n",
    "        encoding_dim = max(int(input_dim * 1.5), 64)\n",
    "        inputs = Input(shape=(input_dim,))\n",
    "        encoded = Dense(encoding_dim, activation=\"relu\")(inputs)\n",
    "        decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "        autoencoder = keras.Model(inputs, decoded)\n",
    "        encoder = keras.Model(inputs, encoded)\n",
    "        early_stop = EarlyStopping(patience=2, monitor=\"loss\")\n",
    "\n",
    "        autoencoder.compile(optimizer='adam', loss=sparse_loss)\n",
    "        autoencoder.fit(X, X, epochs=50, batch_size=256, shuffle=True, callbacks=[early_stop])\n",
    "\n",
    "        return encoder\n",
    "\n",
    "model = SparseAE(X=dataset.X)"
   ],
   "id": "9d3858b2e47bb1b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - loss: 2.1226\n",
      "Epoch 2/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1.3495\n",
      "Epoch 3/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1.2645\n",
      "Epoch 4/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2580\n",
      "Epoch 5/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2552\n",
      "Epoch 6/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2530\n",
      "Epoch 7/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2504\n",
      "Epoch 8/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1.2492\n",
      "Epoch 9/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2483\n",
      "Epoch 10/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2480\n",
      "Epoch 11/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2474\n",
      "Epoch 12/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2472\n",
      "Epoch 13/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1.2470\n",
      "Epoch 14/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1.2466\n",
      "Epoch 15/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1.2464\n",
      "Epoch 16/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2462\n",
      "Epoch 17/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2459\n",
      "Epoch 18/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2457\n",
      "Epoch 19/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2456\n",
      "Epoch 20/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2452\n",
      "Epoch 21/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2451\n",
      "Epoch 22/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2449\n",
      "Epoch 23/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2451\n",
      "Epoch 24/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2448\n",
      "Epoch 25/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2445\n",
      "Epoch 26/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2443\n",
      "Epoch 27/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2442\n",
      "Epoch 28/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2442\n",
      "Epoch 29/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2440\n",
      "Epoch 30/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2438\n",
      "Epoch 31/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2437\n",
      "Epoch 32/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2436\n",
      "Epoch 33/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2435\n",
      "Epoch 34/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2436\n",
      "Epoch 35/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2434\n",
      "Epoch 36/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2432\n",
      "Epoch 37/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2432\n",
      "Epoch 38/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2431\n",
      "Epoch 39/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2430\n",
      "Epoch 40/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2429\n",
      "Epoch 41/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2427\n",
      "Epoch 42/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2429\n",
      "Epoch 43/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2427\n",
      "Epoch 44/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 1.2427\n",
      "Epoch 45/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2425\n",
      "Epoch 46/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2424\n",
      "Epoch 47/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2424\n",
      "Epoch 48/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2423\n",
      "Epoch 49/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2425\n",
      "Epoch 50/50\n",
      "\u001B[1m29/29\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 1.2424\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:55:49.092239Z",
     "start_time": "2025-04-30T08:55:49.087797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class RawDataExperimentDatabase:\n",
    "    \"\"\"\n",
    "    This will enable consistent experiments - Each split of dataset will be save, i.e. the indexes will be saved.\n",
    "    Thus changes to the data will not result in changes to the split\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: RawDataset):\n",
    "        self.dataset = dataset\n",
    "        db_path = os.path.join(\"..\",\"data\", dataset.name)\n",
    "        os.makedirs(db_path, exist_ok=True)\n",
    "        self.db_path = os.path.join(db_path, f\"{dataset.name}_dataset.json\")\n",
    "        self.key = str(1)\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(f\"Path exists: {self.db_path}\")\n",
    "            self.db = json.load(open(self.db_path, \"r\"))\n",
    "            self.empty = False if self.key in self.db else True\n",
    "        else:\n",
    "            self.db = {1: {}}\n",
    "            self.empty = True\n",
    "\n",
    "\n",
    "    def _save(self):\n",
    "        with open(self.db_path, \"w\") as f:\n",
    "            json.dump(self.db, f)\n",
    "\n",
    "    def get_split(self):\n",
    "\n",
    "        X, y = self.dataset.get_dataset()\n",
    "\n",
    "        if self.empty:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y,\n",
    "                                                                random_state=42)\n",
    "\n",
    "\n",
    "            X_sample, y_sample = X_train, y_train\n",
    "\n",
    "\n",
    "            new_data = {}\n",
    "            new_data[\"train_index\"] = X_train.index.tolist()\n",
    "            new_data[\"imm_train_index\"] = X_sample.index.tolist()\n",
    "            new_data[\"test_index\"] = X_test.index.tolist()\n",
    "\n",
    "            self.db[self.key] = new_data\n",
    "\n",
    "            print(f\"#### CREATED NEW INDEX FOR 1 - INDEX SIZE {len(X_sample)}\")\n",
    "\n",
    "            self._save()\n",
    "\n",
    "        else:\n",
    "            indexes = self.db[self.key]\n",
    "            # Get the existing indices and create new dataframes\n",
    "            X_train = pd.DataFrame(X.loc[indexes[\"train_index\"]])\n",
    "            y_train = pd.Series(y.loc[indexes[\"train_index\"]])\n",
    "            X_sample = pd.DataFrame(X.loc[indexes[\"imm_train_index\"]])\n",
    "            y_sample = pd.Series(y.loc[indexes[\"imm_train_index\"]])\n",
    "            X_test = pd.DataFrame(X.loc[indexes[\"test_index\"]])\n",
    "            y_test = pd.Series(y.loc[indexes[\"test_index\"]])\n",
    "            print(f\"LOADED INDEX 1 - INDEX SIZE {len(X_sample)}\")\n",
    "\n",
    "        return X_train.values, X_test.values, X_sample.values, y_train.values, y_test.values, y_sample.values\n"
   ],
   "id": "bd5604df16cc38ce",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:55:49.113974Z",
     "start_time": "2025-04-30T08:55:49.108309Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, X_test, X_sample, y_train, y_test, y_sample = RawDataExperimentDatabase(dataset).get_split()",
   "id": "ab355788b6c8dc4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path exists: ../data/ring/ring_dataset.json\n",
      "LOADED INDEX 1 - INDEX SIZE 6660\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:55:49.122156Z",
     "start_time": "2025-04-30T08:55:49.116641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_emb = model.forward(X_sample)\n",
    "X_test_emb = model.forward(X_test)"
   ],
   "id": "10cca148e080e808",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:55:49.136201Z",
     "start_time": "2025-04-30T08:55:49.131568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.src.metrics import F1Score\n",
    "from keras.src.callbacks import LearningRateScheduler\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.src.models import Model\n",
    "\n",
    "class NeuralNetworkInternalModel(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.batch_size = 64\n",
    "        self.dropout_rate = 0.3\n",
    "        self.epochs = 100\n",
    "        self.model: Model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        lr_scheduler = LearningRateScheduler(lambda epoch: 0.0001 * (0.9 ** epoch))\n",
    "        early_stopping = EarlyStopping(patience=2, monitor='loss')\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = self.model.predict(X)\n",
    "        return np.argmax(prediction, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        if len(y.shape) == 2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "\n",
    "        pred = self.predict(X)\n",
    "        return accuracy_score(y, pred), f1_score(y, pred, average='weighted')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EmbeddingBaseline(NeuralNetworkInternalModel):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.name = \"neural_network\"\n",
    "        num_classes = kwargs.get(\"num_classes\")\n",
    "        input_shape = kwargs.get(\"input_shape\")\n",
    "        self.model = self.get_model(num_classes=num_classes, input_shape=input_shape)\n",
    "\n",
    "    def get_model(self, num_classes, input_shape):\n",
    "\n",
    "        if isinstance(input_shape, int):\n",
    "            input_shape = (input_shape,)\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Flatten()(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(units=128, activation='leaky_relu')(x)\n",
    "        x = Dropout(self.dropout_rate)(x)\n",
    "\n",
    "        # Define the output layer\n",
    "        outputs = Dense(units=num_classes, activation='softmax')(x)\n",
    "\n",
    "        # Create the model\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # Compile the model with F1 Score\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy', F1Score()]\n",
    "                      )\n",
    "\n",
    "        return model\n",
    "\n"
   ],
   "id": "1f99c748877695ac",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:56:09.291064Z",
     "start_time": "2025-04-30T08:55:49.143781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "baseline_model = EmbeddingBaseline(num_classes=2, input_shape=X_train_emb.shape[1])\n",
    "\n",
    "# Convert integer labels to one-hot encoded labels\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=2)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "# Fit the model with one-hot encoded labels\n",
    "baseline_model.fit(\n",
    "    X_train_emb, y_train_one_hot  # Use one-hot encoded labels here\n",
    ")\n"
   ],
   "id": "6996b5b3190e6519",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 9ms/step - accuracy: 0.5586 - f1_score: 0.5579 - loss: 0.7540 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.6969 - f1_score: 0.6968 - loss: 0.5918 - learning_rate: 9.0000e-05\n",
      "Epoch 3/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.7663 - f1_score: 0.7662 - loss: 0.5096 - learning_rate: 8.1000e-05\n",
      "Epoch 4/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8219 - f1_score: 0.8217 - loss: 0.4517 - learning_rate: 7.2900e-05\n",
      "Epoch 5/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8199 - f1_score: 0.8195 - loss: 0.4301 - learning_rate: 6.5610e-05\n",
      "Epoch 6/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8457 - f1_score: 0.8454 - loss: 0.3968 - learning_rate: 5.9049e-05\n",
      "Epoch 7/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8645 - f1_score: 0.8641 - loss: 0.3681 - learning_rate: 5.3144e-05\n",
      "Epoch 8/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8657 - f1_score: 0.8654 - loss: 0.3546 - learning_rate: 4.7830e-05\n",
      "Epoch 9/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8620 - f1_score: 0.8616 - loss: 0.3556 - learning_rate: 4.3047e-05\n",
      "Epoch 10/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8648 - f1_score: 0.8644 - loss: 0.3521 - learning_rate: 3.8742e-05\n",
      "Epoch 11/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8792 - f1_score: 0.8789 - loss: 0.3287 - learning_rate: 3.4868e-05\n",
      "Epoch 12/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8838 - f1_score: 0.8834 - loss: 0.3243 - learning_rate: 3.1381e-05\n",
      "Epoch 13/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8879 - f1_score: 0.8877 - loss: 0.3180 - learning_rate: 2.8243e-05\n",
      "Epoch 14/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8774 - f1_score: 0.8770 - loss: 0.3209 - learning_rate: 2.5419e-05\n",
      "Epoch 15/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8898 - f1_score: 0.8895 - loss: 0.3065 - learning_rate: 2.2877e-05\n",
      "Epoch 16/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8764 - f1_score: 0.8759 - loss: 0.3204 - learning_rate: 2.0589e-05\n",
      "Epoch 17/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8795 - f1_score: 0.8792 - loss: 0.3182 - learning_rate: 1.8530e-05\n",
      "Epoch 18/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8869 - f1_score: 0.8867 - loss: 0.3030 - learning_rate: 1.6677e-05\n",
      "Epoch 19/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8880 - f1_score: 0.8878 - loss: 0.3060 - learning_rate: 1.5009e-05\n",
      "Epoch 20/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8880 - f1_score: 0.8878 - loss: 0.3060 - learning_rate: 1.3509e-05\n",
      "Epoch 21/100\n",
      "\u001B[1m105/105\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8898 - f1_score: 0.8896 - loss: 0.2960 - learning_rate: 1.2158e-05\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:56:09.430793Z",
     "start_time": "2025-04-30T08:56:09.322761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_model.evaluate(\n",
    "    X_test_emb, y_test,\n",
    ")"
   ],
   "id": "34cd56488b415d98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m24/24\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9202702702702703, 0.9201136515503515)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fd6b37045564161d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
